{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFP7RoDzhYnE",
        "outputId": "3bd49eca-a086-4672-c858-05439f2f6bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Prior Probabilities:\n",
            "P(Yes) = 0.6429, P(No) = 0.3571\n",
            "\n",
            "Likelihoods for <Rain, Cool, High, Strong>:\n",
            "Given Yes: {'outlook': 0.3333333333333333, 'temp': 0.3333333333333333, 'humidity': 0.3333333333333333, 'wind': 0.3333333333333333}\n",
            "Given No: {'outlook': 0.4, 'temp': 0.2, 'humidity': 0.8, 'wind': 0.6} \n",
            "\n",
            "Class Conditional Probabilities:\n",
            "P(Yes | X) = 0.3666, P(No | X) = 0.6334\n",
            "\n",
            "Prediction: No\n",
            "\n",
            "Validation using sklearn CategoricalNB:\n",
            "Predicted Class: No\n",
            "Predicted Probabilities: P(Yes | X) = 0.4087, P(No | X) = 0.5913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but OrdinalEncoder was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"play.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Compute class prior probabilities\n",
        "class_counts = df[\"play\"].value_counts()\n",
        "total_samples = len(df)\n",
        "prior_yes = class_counts[\"Yes\"] / total_samples\n",
        "prior_no = class_counts[\"No\"] / total_samples\n",
        "\n",
        "# Function to compute likelihoods\n",
        "def compute_likelihood(feature, value, target_class):\n",
        "    subset = df[df[\"play\"] == target_class]\n",
        "    return len(subset[subset[feature] == value]) / len(subset)\n",
        "\n",
        "# Given test sample: <Rain, Cool, High, Strong>\n",
        "test_sample = {\"outlook\": \"Rain\", \"temp\": \"Cool\", \"humidity\": \"High\", \"wind\": \"Strong\"}\n",
        "\n",
        "# Compute likelihoods P(X|Yes) and P(X|No)\n",
        "likelihoods_yes = {feature: compute_likelihood(feature, value, \"Yes\") for feature, value in test_sample.items()}\n",
        "likelihoods_no = {feature: compute_likelihood(feature, value, \"No\") for feature, value in test_sample.items()}\n",
        "\n",
        "# Compute class conditional probabilities P(Yes|X) and P(No|X)\n",
        "prob_yes_given_x = prior_yes * likelihoods_yes[\"outlook\"] * likelihoods_yes[\"temp\"] * \\\n",
        "                   likelihoods_yes[\"humidity\"] * likelihoods_yes[\"wind\"]\n",
        "prob_no_given_x = prior_no * likelihoods_no[\"outlook\"] * likelihoods_no[\"temp\"] * \\\n",
        "                  likelihoods_no[\"humidity\"] * likelihoods_no[\"wind\"]\n",
        "\n",
        "# Normalize probabilities\n",
        "total_prob = prob_yes_given_x + prob_no_given_x\n",
        "prob_yes_given_x /= total_prob\n",
        "prob_no_given_x /= total_prob\n",
        "\n",
        "# Print results\n",
        "print(\"Class Prior Probabilities:\")\n",
        "print(f\"P(Yes) = {prior_yes:.4f}, P(No) = {prior_no:.4f}\\n\")\n",
        "\n",
        "print(\"Likelihoods for <Rain, Cool, High, Strong>:\")\n",
        "print(\"Given Yes:\", likelihoods_yes)\n",
        "print(\"Given No:\", likelihoods_no, \"\\n\")\n",
        "\n",
        "print(\"Class Conditional Probabilities:\")\n",
        "print(f\"P(Yes | X) = {prob_yes_given_x:.4f}, P(No | X) = {prob_no_given_x:.4f}\\n\")\n",
        "print(\"Prediction:\", \"Yes\" if prob_yes_given_x > prob_no_given_x else \"No\")\n",
        "\n",
        "# Validate using sklearn CategoricalNB\n",
        "encoder = OrdinalEncoder()\n",
        "X_encoded = encoder.fit_transform(df[[\"outlook\", \"temp\", \"humidity\", \"wind\"]])\n",
        "y_encoded = np.where(df[\"play\"] == \"Yes\", 1, 0)\n",
        "\n",
        "nb_model = CategoricalNB()\n",
        "nb_model.fit(X_encoded, y_encoded)\n",
        "\n",
        "test_sample_encoded = encoder.transform([list(test_sample.values())])\n",
        "predicted_class = nb_model.predict(test_sample_encoded)\n",
        "predicted_probs = nb_model.predict_proba(test_sample_encoded)\n",
        "\n",
        "print(\"\\nValidation using sklearn CategoricalNB:\")\n",
        "print(f\"Predicted Class: {'Yes' if predicted_class[0] == 1 else 'No'}\")\n",
        "print(f\"Predicted Probabilities: P(Yes | X) = {predicted_probs[0][1]:.4f}, P(No | X) = {predicted_probs[0][0]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from graphviz import Source\n",
        "from IPython.display import SVG\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"play.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Function to calculate entropy\n",
        "def calculate_entropy(column):\n",
        "    _, counts = np.unique(column, return_counts=True)\n",
        "    return entropy(counts, base=2)\n",
        "\n",
        "# Compute information gain for each attribute\n",
        "def information_gain(df, feature, target):\n",
        "    total_entropy = calculate_entropy(df[target])\n",
        "    values, counts = np.unique(df[feature], return_counts=True)\n",
        "    weighted_entropy = sum((counts[i] / sum(counts)) * calculate_entropy(df[df[feature] == values[i]][target]) for i in range(len(values)))\n",
        "    return total_entropy - weighted_entropy\n",
        "\n",
        "# Compute information gain for all attributes\n",
        "info_gains = {feature: information_gain(df, feature, \"play\") for feature in [\"outlook\", \"temp\", \"humidity\", \"wind\"]}\n",
        "\n",
        "# Find the root node (attribute with max information gain)\n",
        "root_node = max(info_gains, key=info_gains.get)\n",
        "\n",
        "# Print information gain values\n",
        "print(\"Information Gain for each attribute:\")\n",
        "for feature, gain in info_gains.items():\n",
        "    print(f\"{feature}: {gain:.4f}\")\n",
        "print(f\"\\nRoot node based on Information Gain: {root_node}\\n\")\n",
        "\n",
        "# Encode categorical features for DecisionTreeClassifier\n",
        "encoder = OrdinalEncoder()\n",
        "X_encoded = encoder.fit_transform(df[[\"outlook\", \"temp\", \"humidity\", \"wind\"]])\n",
        "y_encoded = np.where(df[\"play\"] == \"Yes\", 1, 0)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "id3_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
        "id3_model.fit(X_encoded, y_encoded)\n",
        "\n",
        "# Classify test sample <Rain, Cool, High, Weak>\n",
        "test_sample = encoder.transform([[\"Rain\", \"Cool\", \"High\", \"Weak\"]])\n",
        "predicted_class = id3_model.predict(test_sample)\n",
        "print(\"Predicted Class for <Rain, Cool, High, Weak>:\", \"Yes\" if predicted_class[0] == 1 else \"No\")\n",
        "\n",
        "# Visualize Decision Tree\n",
        "graph = Source(export_graphviz(id3_model, feature_names=[\"outlook\", \"temp\", \"humidity\", \"wind\"], class_names=[\"No\", \"Yes\"], filled=True))\n",
        "SVG(graph.pipe(format='svg'))\n",
        "\n",
        "# Check if the root node matches\n",
        "scikit_root_node = [\"outlook\", \"temp\", \"humidity\", \"wind\"][id3_model.tree_.feature[0]]\n",
        "print(f\"Root node from Scikit DecisionTreeClassifier: {scikit_root_node}\")\n",
        "print(\"Match with computed root node:\", root_node == scikit_root_node)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zezaYAoOiney",
        "outputId": "fbef06ea-ec33-40d8-d347-197bc8f21672"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information Gain for each attribute:\n",
            "outlook: 0.2467\n",
            "temp: 0.0292\n",
            "humidity: 0.1518\n",
            "wind: 0.0481\n",
            "\n",
            "Root node based on Information Gain: outlook\n",
            "\n",
            "Predicted Class for <Rain, Cool, High, Weak>: No\n",
            "Root node from Scikit DecisionTreeClassifier: outlook\n",
            "Match with computed root node: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but OrdinalEncoder was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load datasets\n",
        "diabetes_file = \"diabetes.csv\"\n",
        "iris_file = \"Iris.csv\"\n",
        "\n",
        "diabetes_df = pd.read_csv(diabetes_file)\n",
        "iris_df = pd.read_csv(iris_file)\n",
        "\n",
        "### Diabetes Prediction ###\n",
        "# Prepare data\n",
        "X_diabetes = diabetes_df.iloc[:, :-1]\n",
        "y_diabetes = diabetes_df.iloc[:, -1]\n",
        "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_diabetes, y_diabetes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train_d = scaler.fit_transform(X_train_d)\n",
        "X_test_d = scaler.transform(X_test_d)\n",
        "\n",
        "# Train and evaluate Naïve Bayes\n",
        "nb_diabetes = GaussianNB()\n",
        "nb_diabetes.fit(X_train_d, y_train_d)\n",
        "y_pred_nb_d = nb_diabetes.predict(X_test_d)\n",
        "\n",
        "# Train and evaluate Decision Tree\n",
        "dt_diabetes = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)\n",
        "dt_diabetes.fit(X_train_d, y_train_d)\n",
        "y_pred_dt_d = dt_diabetes.predict(X_test_d)\n",
        "\n",
        "print(\"Diabetes Prediction:\")\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test_d, y_pred_nb_d))\n",
        "print(classification_report(y_test_d, y_pred_nb_d))\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test_d, y_pred_dt_d))\n",
        "print(classification_report(y_test_d, y_pred_dt_d))\n",
        "\n",
        "### Iris Classification ###\n",
        "# Prepare data\n",
        "X_iris = iris_df.iloc[:, 1:-1]\n",
        "y_iris = iris_df.iloc[:, -1]\n",
        "le = LabelEncoder()\n",
        "y_iris = le.fit_transform(y_iris)  # Convert species names to numerical labels\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate Naïve Bayes\n",
        "nb_iris = GaussianNB()\n",
        "nb_iris.fit(X_train_i, y_train_i)\n",
        "y_pred_nb_i = nb_iris.predict(X_test_i)\n",
        "\n",
        "# Train and evaluate Decision Tree\n",
        "dt_iris = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)\n",
        "dt_iris.fit(X_train_i, y_train_i)\n",
        "y_pred_dt_i = dt_iris.predict(X_test_i)\n",
        "\n",
        "print(\"Iris Classification:\")\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test_i, y_pred_nb_i))\n",
        "print(classification_report(y_test_i, y_pred_nb_i, target_names=le.classes_))\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test_i, y_pred_dt_i))\n",
        "print(classification_report(y_test_i, y_pred_dt_i, target_names=le.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcVO5U1Hjugf",
        "outputId": "8f35bab4-ae50-4e83-bca5-8f99c843c2f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes Prediction:\n",
            "Naïve Bayes Accuracy: 0.7662337662337663\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.80      0.81        99\n",
            "           1       0.66      0.71      0.68        55\n",
            "\n",
            "    accuracy                           0.77       154\n",
            "   macro avg       0.75      0.75      0.75       154\n",
            "weighted avg       0.77      0.77      0.77       154\n",
            "\n",
            "Decision Tree Accuracy: 0.7857142857142857\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.84      0.83        99\n",
            "           1       0.70      0.69      0.70        55\n",
            "\n",
            "    accuracy                           0.79       154\n",
            "   macro avg       0.77      0.76      0.77       154\n",
            "weighted avg       0.78      0.79      0.79       154\n",
            "\n",
            "Iris Classification:\n",
            "Naïve Bayes Accuracy: 1.0\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      1.00      1.00        10\n",
            "Iris-versicolor       1.00      1.00      1.00         9\n",
            " Iris-virginica       1.00      1.00      1.00        11\n",
            "\n",
            "       accuracy                           1.00        30\n",
            "      macro avg       1.00      1.00      1.00        30\n",
            "   weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Decision Tree Accuracy: 1.0\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "    Iris-setosa       1.00      1.00      1.00        10\n",
            "Iris-versicolor       1.00      1.00      1.00         9\n",
            " Iris-virginica       1.00      1.00      1.00        11\n",
            "\n",
            "       accuracy                           1.00        30\n",
            "      macro avg       1.00      1.00      1.00        30\n",
            "   weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}